{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039bc9aa",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymc3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymc3\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpm\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wishart, uniform, gamma, multivariate_normal \u001b[38;5;28;01mas\u001b[39;00m mv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymc3'"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc3 as pm\n",
    "import random\n",
    "from scipy.stats import wishart, uniform, gamma, multivariate_normal as mv\n",
    "from scipy.stats import differential_entropy as dif_en\n",
    "from scipy.linalg import det\n",
    "from scipy.integrate import nquad\n",
    "from math import pi, e\n",
    "\n",
    "# this is the package with the joint class 'info' for all information theoretical measures\n",
    "import nn_info as nit\n",
    "# this is the collection of information theoretical algorithms based on nearest neighbors with detailed prescription\n",
    "import estimators as est\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d9864",
   "metadata": {},
   "source": [
    "# 0. Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1920a",
   "metadata": {},
   "source": [
    "## 0.1. Entropy estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52cc479",
   "metadata": {},
   "source": [
    "### 0.1.1 Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe2271",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Gaussian example for entropy estimation:\n",
    "#try different dimension via choice of 'd':\n",
    "d = 2\n",
    "# sample size\n",
    "n = 1000\n",
    "# number of estimates for estimate mean values computation\n",
    "m = 5\n",
    "\n",
    "s = 0.5*np.ones(d**2).reshape(-1,d)\n",
    "I = np.identity(d)\n",
    "s = s + I\n",
    "\n",
    "Res=[]\n",
    "for j in range(50):\n",
    "    #make Wishart for pos.def. covariance matrice  \n",
    "    S = wishart.rvs(df=10, scale = s)\n",
    "    \n",
    "    res = []\n",
    "    res2 = []\n",
    "    #compute mean of  KLD estimators (see nnInfo for detailed information)\n",
    "    for i in range(m):\n",
    "        X = mv.rvs(np.zeros(d), cov=S, size=n)\n",
    "        entropy = nit.info('entropy', approach = 'nn_distance', k = 1, unit = 'nats')\n",
    "        res.append(entropy.estimator(X))\n",
    "        res2.append(dif_en(X))\n",
    "    if d > 1:\n",
    "        Res.append([np.mean(res), np.std(res), np.mean(res2), np.std(res2), 0.5*np.log(2*e*pi*det(S))])\n",
    "    else:\n",
    "        Res.append([np.mean(res), np.std(res), np.mean(res2), np.std(res2), 0.5*(np.log(2*pi*S) + 1)])\n",
    "\n",
    "F = pd.DataFrame(Res, columns=['estimate', 'std_var', 'scipy_est', 'scipy_std', 'truth'])\n",
    "F.sort_values(by='truth', inplace=True)\n",
    "F.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75347c4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#make plot\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "plt.errorbar(range(len(F)), F.estimate, F.std_var, label='KozLeo estimate')\n",
    "plt.errorbar(range(len(F)), F.scipy_est, F.scipy_std, label='Scipy kernel estimate')\n",
    "plt.plot(range(len(F)), F.truth, label='analytical entropy', color='black')\n",
    "plt.xlabel('estimates wrt. different Wishart covariance matrices')\n",
    "plt.ylabel('entropy in nats')\n",
    "plt.title('mean value of ' + str(m) + ' estimates\\n with sample size ' + str(n) + ' and dimension ' + str(d))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115893d",
   "metadata": {},
   "source": [
    "## 0.2. Kullback-Leibler divergence estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049086a8",
   "metadata": {},
   "source": [
    "### 0.2.1 Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ed745",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Gaussian example with variable covariance matrix, sampled from Wishart distribution\n",
    "#dimension\n",
    "d = 3\n",
    "#center of Gaussian: change center for Gaussian to play with intensity of 0-value inflation\n",
    "# negative values will be mapped to 0\n",
    "c = 2.5\n",
    "# amount of computations for mean value\n",
    "m = 5\n",
    "# sample size\n",
    "n = 1000\n",
    "\n",
    "\n",
    "s = 0.5*np.ones(d**2).reshape(-1,d)\n",
    "I = np.identity(d)\n",
    "s = s + I\n",
    "\n",
    "E=[]\n",
    "\n",
    "for j in range(50):\n",
    "    #make 2 Wisharts for pos.def. covariance matrices  \n",
    "    S = wishart.rvs(df=10, scale = s, size = 2)\n",
    "    \n",
    "    e = []\n",
    "    #compute mean of 3 KLD estimators (see nnInfo for detailed information)\n",
    "    for i in range(m):\n",
    "        X = mv.rvs(c*np.ones(d), cov=S[0], size=n)\n",
    "        Y = mv.rvs(c*np.ones(d), cov=S[1], size=n)\n",
    "        e.append((nit.info('KLD', approach='nn_ratio', unit='nats', k=3).estimator((X,Y)),\n",
    "                  nit.info('KLD', approach='nn_distance', unit='nats', k=4, epsilon=False).estimator((X,Y)), \n",
    "                  # the Noshad approach is not part of the class but can be accessed directly\n",
    "                  est.NMI((X,Y), k = 'adapt'),\n",
    "                  nit.KLD_gaussian((X,Y))))\n",
    "        \n",
    "    estimates = np.array(e).mean(axis=0)\n",
    "    #compute true values based on parameter, parameter estimation and \n",
    "    #arithmetic mean estimate \n",
    "    \n",
    "    truth = nit.KLD_gaussian(tpl=False, mu0=np.zeros(d), \n",
    "                             mu1=np.zeros(d), cov0=S[0], cov1=S[1], samples=False)\n",
    "    \n",
    "    E.append(list(estimates) + [truth])\n",
    "F = pd.DataFrame(E, columns=['BannMI', 'WMI', 'NMI','sample estimate', \n",
    "                             'truth'])\n",
    "F.sort_values(by='truth', inplace=True)\n",
    "F.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c1b74",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# make plot\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "plt.plot(range(len(F)), F.BannMI, label='BannMI estimate (nn_ratio)')\n",
    "plt.plot(range(len(F)), F.WMI, label='WMI estimate (nn_distance)')\n",
    "plt.plot(range(len(F)), F.NMI, label='NMI estimate')\n",
    "plt.plot(range(len(F)), F.loc[:,['sample estimate']], label='sample estimate')\n",
    "plt.plot(range(len(F)), F.truth, label='analytical value')\n",
    "plt.xlabel('estimates wrt. different Wishart covariance matrices')\n",
    "plt.ylabel('Kullback-Leibler divergence in nats')\n",
    "plt.title('mean value of ' + str(m) + ' estimates \\n with sample size ' + str(n) + ' and dimension ' + str(d))\n",
    "# some values are very high - therefore restrict y-axis for better visualization\n",
    "plt.ylim(0,3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc973a2",
   "metadata": {},
   "source": [
    "#### correlation of estimators to analytical value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0301f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# show correlation to analytical value\n",
    "print(F.iloc[:-5,:].corr().sort_values('truth', ascending=False).iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4251351",
   "metadata": {},
   "source": [
    "#### data with 0-inflations cause NaN-values for WMI estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69391891",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# set negative realizations of Gaussian to 0 and thus create 0-inflation as in phosphoproteomic data  \n",
    "# suppress RuntimeWarning caused by division by zero due to 0-distance in WMI\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "\n",
    "#\n",
    "Z0 = X.copy()\n",
    "Z0[Z0<0]=0\n",
    "Z1 = Y.copy()\n",
    "Z1[Z1<0]=0\n",
    "\n",
    "table=pd.DataFrame([[nit.info('KLD', 'nn_ratio', unit='nats').estimator((X,Y)),\n",
    "                     nit.info('KLD', 'nn_distance', unit='nats').estimator((X,Y))], \n",
    "                    [nit.info('KLD', 'nn_ratio', unit='nats').estimator((Z0,Z1)),\n",
    "                     nit.info('KLD', 'nn_distance', unit='nats').estimator((Z0,Z1))]], \n",
    "                   index=['normal', 'with 0-inflation'], columns=['BannMI','WMI'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9872e",
   "metadata": {},
   "source": [
    "### 0.2.2 Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995bb76e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# compute KLD between two Gamma varaibles with random alpha and beta parameter \n",
    "# computations for estimator mean value\n",
    "m = 5\n",
    "# sample size\n",
    "n = 1000\n",
    "\n",
    "E=[]\n",
    "\n",
    "for j in range(50):\n",
    "    #make positive parameters for gamma distributions \n",
    "    a = np.random.rand(2)\n",
    "    b = 2*np.random.rand(2)\n",
    "    b = [3,6]\n",
    "    e = []\n",
    "    #compute mean of 3 KLD estimators (see nnInfo for detailed information)\n",
    "    for i in range(m):\n",
    "        X = gamma.rvs(a[0], scale=1/b[0], size=n)\n",
    "        Y = gamma.rvs(a[1], scale=1/b[1], size=n)\n",
    "        e.append((nit.info('KLD', 'nn_ratio', unit='nats').estimator((X,Y)),\n",
    "                  nit.info('KLD', 'nn_distance', unit='nats').estimator((X,Y)),\n",
    "                  est.WMI((X,Y), epsilon=True),\n",
    "                  est.NMI((X,Y), adapt=''),\n",
    "                  nit.KLD_gamma((X,Y))))\n",
    "        \n",
    "    estimates = np.array(e).mean(axis=0)\n",
    "    #compute true values based on parameter, parameter estimation and \n",
    "    #arithmetic mean estimate \n",
    "    \n",
    "    truth = nit.KLD_gamma(tpl=False, a0=a[0], a1=a[1], b0=b[0], b1=b[1],\n",
    "                          samples=False)\n",
    "    \n",
    "    E.append(list(estimates) + [truth])\n",
    "F = pd.DataFrame(E, columns=['BannMI', 'WMI', 'WMI eps', 'NMI', 'sample estimate', 'truth'])\n",
    "F.sort_values(by='truth', inplace=True)\n",
    "F.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4ec1b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# make plot\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "plt.plot(range(len(F)), F.BannMI, label='BannMI estimate')\n",
    "plt.plot(range(len(F)), F.WMI, label='WMI estimate')\n",
    "plt.plot(range(len(F)), F.loc[:,['WMI eps']], label='WMI eps estimate')\n",
    "plt.plot(range(len(F)), F.NMI, label='NMI estimate')\n",
    "plt.plot(range(len(F)), F.loc[:,['sample estimate']], label='sample estimate')\n",
    "#plt.plot(range(len(F)), F.loc[:,['density estimate']], label='density estimate')\n",
    "plt.plot(range(len(F)), F.truth, label='analytical value')\n",
    "plt.xlabel('estimates wrt. different Gamma parameters')\n",
    "plt.ylabel('Kullback-Leibler divergence in nats')\n",
    "plt.title('mean value of ' + str(m) + ' estimates \\n with sample size ' + str(n))\n",
    "# restrict y-lim for better visualization\n",
    "plt.ylim(0,10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b6311",
   "metadata": {},
   "source": [
    "#### correlation of estimators to analytical value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199264d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# correlation to analytical values\n",
    "print(F.iloc[:-5,:].corr().sort_values('truth', ascending=False).iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c75c5",
   "metadata": {},
   "source": [
    "## 0.3. Mutual information estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c558b65",
   "metadata": {},
   "source": [
    "### 0.3.1 Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b434817",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Gaussian example with variable covariance matrix, sampled from Wishart distribution\n",
    "#dimension\n",
    "d = 5\n",
    "# computations for estimator mean value\n",
    "m = 5\n",
    "# sample size\n",
    "n = 1000\n",
    "\n",
    "s = 0.5*np.ones(d**2).reshape(-1,d)\n",
    "I = np.identity(d)\n",
    "s = s + I\n",
    "\n",
    "E=[]\n",
    "\n",
    "for j in range(50):\n",
    "    #make 2 Wisharts for pos.def. covariance matrices  \n",
    "    S = wishart.rvs(df=10, scale = s, size = 1)\n",
    "    S0 = S.copy()\n",
    "    S0[1:,0]=0\n",
    "    S0[0,1:]=0\n",
    "    \n",
    "    e = []\n",
    "    #compute mean of 3 KLD estimators (see nnInfo for detailed information)\n",
    "    for i in range(m):\n",
    "        X = mv.rvs(np.ones(d), cov=S, size=n)\n",
    "        x = X[:,0].copy()\n",
    "        np.random.shuffle(x)\n",
    "        Z = np.concatenate((x.reshape(-1,1),X[:,1:]), axis=1)\n",
    "        \n",
    "        e.append((nit.info('MI', unit='nats').estimator((X[:,0],X[:,1:])),\n",
    "                  # Wang is not implemented in the class as MI estimator\n",
    "                  est.WMI((X[:,0],X[:,1:]), measure='MI', k=4, unit='nats'),\n",
    "                  est.NMI2((X[:,0],X[:,1:]), measure='MI', k='adapt', unit='nats'), \n",
    "                  nit.info('MI', approach='nn_distance', unit='nats').estimator((X[:,0],X[:,1:])),\n",
    "                  nit.KLD_gaussian((X,Z))))\n",
    "        \n",
    "    estimates = np.array(e).mean(axis=0)\n",
    "    #compute true values based on parameter, parameter estimation and \n",
    "    #arithmetic mean estimate \n",
    "    truth = nit.KLD_gaussian(tpl=False, mu0=np.zeros(d), \n",
    "                             mu1=np.zeros(d), cov0=S, cov1=S0, samples=False)\n",
    "    \n",
    "    E.append(list(estimates) + [truth])\n",
    "F = pd.DataFrame(E, columns=['BannMI', 'WMI', 'NMI', 'KSG', 'sample estimate','truth'])\n",
    "F.sort_values(by='truth', inplace=True)\n",
    "F.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66941a4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# make plot\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "plt.plot(range(len(F)), F.BannMI, label='BannMI estimate (nn_ratio)')\n",
    "plt.plot(range(len(F)), F.WMI, label='WMI estimate')\n",
    "plt.plot(range(len(F)), F.NMI, label='NMI estimate')\n",
    "plt.plot(range(len(F)), F.KSG, label='KSG estimate (nn_distance)')\n",
    "plt.plot(range(len(F)), F.loc[:,['sample estimate']], label='sample estimate')\n",
    "#plt.plot(range(len(F)), F.loc[:,['density estimate']], label='density estimate')\n",
    "plt.plot(range(len(F)), F.truth, label='analytical value')\n",
    "plt.xlabel('estimates wrt. different Wishart covariance matrices')\n",
    "plt.ylabel('Mutual information in nats')\n",
    "plt.title('mean value of ' + str(m) + ' estimates \\n with sample size ' + str(n) + ' and dimension ' + str(d))\n",
    "#plt.ylim(0,10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe323b22",
   "metadata": {},
   "source": [
    "#### correlation of estimators to analytical value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c9865",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# correlation to analytical value\n",
    "print(F.iloc[:-5,:].corr().sort_values('truth', ascending=False).iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b5535",
   "metadata": {},
   "source": [
    "### 0.3.2. bivariate Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c653fb",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#this file produces benchmark results for bivariate exponential as in the paper\n",
    "# in some cases, although available on conda, pymc3 has to be installed via pip\n",
    "print('prints 9 progress bars (for each dependency values D):')\n",
    "# suppress pymc3 logger and only show progress bar\n",
    "import logging\n",
    "logger = logging.getLogger('pymc3')\n",
    "logger.setLevel(logging.ERROR)\n",
    "logger.propagate = False\n",
    "\n",
    "\n",
    "MIs = []\n",
    "SDs = []\n",
    "# dependeny parameter, number of computations for mean value (chains)\n",
    "D = np.arange(0.1,1,step=0.1)\n",
    "# number of chains\n",
    "c = 5\n",
    "\n",
    "#sample from bivariate distribution, as proposed by Gumble et al.\n",
    "for delta in D:\n",
    "    def logp(x,y,delta = delta):\n",
    "        return np.log(1+(x+y-1)*delta + x*y*delta**2)-(x+y+x*y*delta)\n",
    "         \n",
    "    with pm.Model() as model:\n",
    "        x = pm.Exponential('x', lam=1)\n",
    "        y = pm.Exponential('y', lam=1)\n",
    "        mv_exp = pm.DensityDist('mv_exp', logp, \n",
    "                                observed = dict(x=x, y=y))\n",
    "        trace = pm.sample(tune=2000, draws=1000, chains=c,\n",
    "                          return_inferencedata=True, target_accept=0.9, \n",
    "                          idata_kwargs={\"density_dist_obs\": False})\n",
    "        \n",
    "    \n",
    "    # compute MI for all traces\n",
    "    mis = []\n",
    "    for i in range(c):\n",
    "        y = (np.array(trace['posterior']['x'][i,:]).reshape(-1,1),\n",
    "             np.array(trace['posterior']['y'][i,:]).reshape(-1,1))\n",
    "        mis.append((nit.info('MI', approach='nn_distance', unit='nats').estimator(y),\n",
    "                    nit.info('MI', unit='nats').estimator(y),\n",
    "                    nit.info('MI', unit='nats', prior=1).estimator(y),\n",
    "                    est.WMI(y, measure='MI', k=4, unit='nats'),\n",
    "                    est.NMI2(y, measure='MI', k = 'adapt')))\n",
    "    estimates = np.array(mis).mean(axis=0)\n",
    "    estimates_sd = np.array(mis).std(axis=0)\n",
    "    MIs.append(estimates)\n",
    "    SDs.append(estimates_sd)\n",
    "\n",
    "cols =  ['KSG', 'BannMI','uBannMI', 'WMI', 'NMI']   \n",
    "MI = pd.DataFrame(MIs, columns=cols, index=D)\n",
    "SD = pd.DataFrame(SDs, columns=cols, index=D)\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4c2bc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# numeric integration to compute MI for biariate exponential variable with componentwise dependency\n",
    "truth = []\n",
    "for delta in D:\n",
    "    def fxy(x,y):\n",
    "        return (1+(x+y-1)*delta + x*y*delta**2)*np.exp(-x-y-x*y*delta)\n",
    "    \n",
    "    def fxfy(x,y):\n",
    "        return np.exp(-x-y)\n",
    "    # KLD formula is shortened wrt. fxy/fxfy\n",
    "    def KLD(x,y):\n",
    "        return (np.log(1 + (x+y-1)*delta + x*y*delta**2) - delta*x*y)*fxy(x,y)\n",
    "    \n",
    "    # x-axis value until where integration takes place to ensure convergence\n",
    "    m = 100\n",
    "    result = nquad(KLD, [[0,m],[0,m]]) \n",
    "    truth.append(result[0])\n",
    "# add numeric 'truth'    \n",
    "MI['numeric estimate'] = truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a67641",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# make plot\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "for i in range(5):\n",
    "    ax.errorbar(np.random.normal(np.array(D),0.01),MI.iloc[:,i], SD.iloc[:,i], label=cols[i])\n",
    "ax.plot(np.array(D),MI.iloc[:,-1], label='numeric estimate')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84603d71",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#### correlation of estimators to numerical value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9024095",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# correlation to numeric value\n",
    "print(MI.corr().sort_values('numeric estimate', ascending=False).iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f97855",
   "metadata": {},
   "source": [
    "# 1. Motivational example for 2-to-1 dependency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d442440",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# example as in BannMI paper\n",
    "cor = []\n",
    "uncor = []\n",
    "for i in range(50):\n",
    "    # 2D uniform\n",
    "    x = uniform.rvs(size = (5000,2)) \n",
    "    \n",
    "    #evaluation pdfs of 2D gaussians on uniform sample \n",
    "    z = np.random.normal(mv([0,0],[[1,0.95],[0.95,1]]).pdf(x),0.01)\n",
    "    z_star = np.random.normal(mv([0,0],[[1,0],[0,1]]).pdf(x),0.01)\n",
    "\n",
    "\n",
    "    cor.append([nit.info('MI').estimator((x,z)),\n",
    "                nit.info('MI').estimator((x[:,0],z)), \n",
    "                np.corrcoef((x[:,0],z))[0,1]])\n",
    "                \n",
    "    uncor.append([nit.info('MI').estimator((x,z_star)),\n",
    "                  nit.info('MI').estimator((x[:,0],z_star)), \n",
    "                  np.corrcoef((x[:,0],z_star))[0,1]])\n",
    "\n",
    "\n",
    "ind = ['joint MI', 'univ. MI', 'cor'] \n",
    "Cor = pd.DataFrame(cor, columns = ind)\n",
    "Uncor = pd.DataFrame(uncor, columns = ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6797a0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# make plot \n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(8,3))\n",
    "ax2.scatter(x[:,0], x[:,1], c = z_star)\n",
    "ax2.set_title('uncorrelated Gaussian')\n",
    "ax1.scatter(x[:,0], x[:,1], c = z)\n",
    "ax1.set_title('correlated Gaussian')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62b75a",
   "metadata": {},
   "source": [
    "#### increased information about 'color'-variable in correlated setting can only be captures via joint MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c4db3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# show different measures to capture dependency between x- and y-axis and color scheme\n",
    "C = pd.concat([Cor.mean(axis= 0), Uncor.mean(axis= 0)], axis = 1)\n",
    "C.columns= ['correlated', 'uncorrelated']\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e2779",
   "metadata": {},
   "source": [
    "# 2. Application on cell lines: phosphoproteomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9891de",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compute MI between kinases and an apoptotic marker to determine their potential role in apoptosis\n",
    "\n",
    "# cell lines are adjusted to 0 as smallest value and reduced to EGF stimulation only\n",
    "control = ['MCF12A', '184B5']\n",
    "cancer =  ['HBL100', 'HCC3153']\n",
    "lines = control + cancer\n",
    "\n",
    "\n",
    "Lines=[]\n",
    "t = []\n",
    "m = 25\n",
    "\n",
    "\n",
    "for n,i in enumerate(lines):\n",
    "    x = pd.read_csv('data/' + i + '.csv')\n",
    "    t = t + list(np.unique(x.time))\n",
    "    Lines.append(x)\n",
    "\n",
    "# efa as 'affector for apoptosis'\n",
    "efas = ['p.SMAD23', 'p.AMPK', 'p.JNK', 'p.STAT1']\n",
    "# indicator of apoptosis\n",
    "marker = ['cleavedCas']\n",
    "\n",
    "time = np.unique(t)\n",
    "\n",
    "MI = []\n",
    "for l in Lines:\n",
    "    MI_mean = []\n",
    "    MI_sd = []\n",
    "    for t in time:\n",
    "        X = l.loc[l.time==t,:]\n",
    "        mis = []\n",
    "        for j in range(m):\n",
    "            x = X.sample(500, axis=0)\n",
    "            mis.append(nit.info('MI').estimator((x.loc[:,marker], x.loc[:,efas])))\n",
    "\n",
    "        MI_mean.append(np.mean(mis))  \n",
    "        MI_sd.append(np.std(mis))  \n",
    "    MI.append((MI_mean, MI_sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9932f6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot MI for each line\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "for j in range(4):\n",
    "    if j == 0:\n",
    "        ax.errorbar(time, MI[j][0], MI[j][1], color='black', label='control')\n",
    "    if j == 1: \n",
    "        ax.errorbar(time, MI[j][0], MI[j][1], color='black')\n",
    "    if j == 2: \n",
    "        ax.errorbar(time, MI[j][0], MI[j][1], color='red', label='cancer')\n",
    "    if j == 3:    \n",
    "        ax.errorbar(time, MI[j][0], MI[j][1], color='red')\n",
    "ax.set_xticks(time)\n",
    "ax.set_xticklabels(time, rotation=90)\n",
    "ax.set_title('Dependencies (MI) of 2 control and 2 cancer lines \\n between phosphoproteins and a marker for apoptosis')\n",
    "ax.set_ylabel('MI in bits')\n",
    "ax.set_xlabel('time after EGF stimulation')\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aebe98",
   "metadata": {},
   "source": [
    "#### References:\n",
    "**KozLeo**: Kozachenko, L. and Leonenko, N. (1987). Sample Estimate of the Entropy\n",
    "of a Random Vector. Problems of Information Transmission, 23(2),\n",
    "95–101.\n",
    "\n",
    "**WMI**: Wang, Q. et al. (2009). Divergence Estimation for Multidimensional\n",
    "Densities Via k-Nearest-Neighbor Distances. IEEE Transactions of\n",
    "Information Theory, 55(5), 2392–2405.\n",
    "\n",
    "**NMI**: Noshad, M. et al. (2017). Direct estimation of information divergence\n",
    "using nearest neighbor ratios. In 2017 IEEE International Symposium\n",
    "on Information Theory (ISIT 2017)\", pages 903 – 907, Aachen, Germany.\n",
    "Institute of Electrical and Electronics Engineers ( IEEE ).\n",
    "\n",
    "**KSG**: Kraskov, A. et al. (2004). Estimating mutual information. Phys. Rev. E,\n",
    "69, 066138.\n",
    "\n",
    "**(u)BannMI**: Schmidt, B. et al. BannMI Deciphers Potential n-to-1 Information\n",
    "Transduction in Signalling Pathways to Unravel\n",
    "Message of Intrinsic Apoptosis\n",
    "\n",
    "cell lines used in **2. Application on cell lines** are contributed by Attila Gabor and Marco Tognetti. They were obtained as part of the Single Cell Signaling in Breast Cancer Challenge through Synapse ID syn20564742\n",
    "\n",
    "**bivariate exponential distribution** was proposed by: Gumbel, E. J. (1960). Bivariate exponential distributions. Journal of the American Statistical Association, 55(292), 698–707."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
